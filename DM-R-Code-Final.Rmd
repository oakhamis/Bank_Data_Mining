---
title: "DATA MINING CLASSIFICATION TECHNIQUES PREDICTING TERM DEPOSIT INVESTORS"
author: "Omar Khamis"
date: "11/01/2021"
output:
  pdf_document:
    toc: yes
    number_sections: yes
header-includes: \usepackage{setspace}\doublespacing
fontsize: 12pt
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```
\newpage

# DOMAIN DESCRIPTION

The effects of the 2008 financial crisis led banks to implement more credit restrictions, increase deposit retention, and demand improvements in the effectiveness of direct marketing campaigns (Moro et al, 2012). Direct marketing is a targeted method of marketing which collects, examines, and utilizes data about customers who are likely to be interested in a specific product or service (Thomas and Housden, 2002). 
The ineptitudes of the mass marketing method had more banks targeting their clients using the direct marketing method in order to increase sales of their products and services (Patel and Mazumdar, 2018). Data mining techniques can enhance direct marketing by pinpointing the main characteristics or attributes that affects the outcome of the campaign which would help classify the customer profile that needs focused attention and resources (Moro et al, 2011).
Data mining is the use of statistical algorithms to uncover relationships and links between variables in a dataset (Pavlović et al, 2014). Classification is a data mining model type that is used to make a group affiliation prediction for a data instance which helps assign unlabeled new data to an output class variable with a certain accuracy (Wisaeng, 2013). 

# PROBLEM DEFINITION

The classification problem to be tackled is to build models that predict whether clients will invest in a bank term deposit given a number of attributes in the dataset of a direct marketing campaign. In this report, three classification techniques will used: K-Nearest Neighbor, Decision Tree, and Naïve Bayes Classifiers. The main challenge is finding the technique that will best balance giving high accuracy, low complexity, and logically inferable results. A comparison will be made between these three techniques and then the results of the model’s algorithms will be interpreted to argue which technique will be best suited for this application. 

# DATASET DESCRIPTION
 
The dataset used in this report consists of 45,211 records and 17 attributes which includes customer information of a Portuguese retail bank (Moro et al, 2014). This data includes the results of a direct marketing campaign and it was collected by making phone calls to bank clients. The goal of the campaign was to predict whether the client will subscribe to a term deposit or not. Table 1 below describes the 16 input variables that include the customer information and 1 output variable that shows if the client agreed to invest in a term deposit.


```{r data}
bankdata = read.table('bank-full.csv',sep=',',header = T)
```

__TABLE 1: DATASET DESCRIPTION__
```{r table}
datadesc = read.table('DatasetTable.csv',sep=',',header = T)
colnames(datadesc) = c('ATTRIBUTE NAME', 'ATTRIBUTE DESCRITPTION (CATEGORY VALUES)','DATA TYPE')
kable(datadesc) 
```
\newpage

# DATASET PRE-PROCESSING

* Check if there are missing values.
```{r  Missing}
    sapply(bankdata, function(x) sum(is.na(x)))
```
*	Check if there are duplicated values.
```{r  Duplicated}
sum(duplicated(bankdata))
```

*	Change all data type of all the categorical variables to factors and then to numeric to process into the KNN classification algorithm.

```{r  preprocessing}
bankdataP <- data.frame(bankdata$age,as.numeric(as.factor(bankdata$job)),as.numeric(as.factor(bankdata$marital)),as.numeric(as.factor(bankdata$education)),as.numeric(as.factor(bankdata$default)),bankdata$balance,as.numeric(as.factor(bankdata$housing)),as.numeric(as.factor(bankdata$loan)),as.numeric(as.factor(bankdata$contact)),bankdata$day,as.numeric(as.factor(bankdata$month)),bankdata$duration,bankdata$campaign,bankdata$pdays,bankdata$previous,as.numeric(as.factor(bankdata$poutcome)),as.numeric(as.factor(bankdata$y)))
colnames(bankdataP)=c("Age","job","marital","education","default","balance","housing","loan","contact","day","month","duration","campaign","pdays","previous","poutcome","y")                                                  
```

*	Splitting the dataset to a training and testing data by a 70 and 30 respective split.

```{r train-test}
library(class)
library(e1071)
library(caret)
library(rpart.plot)
set.seed(0)

intrain <- createDataPartition(y = bankdataP$y, p= 0.7, list = FALSE)
training <- bankdataP[intrain,]
testing <- bankdataP[-intrain,]

dim(training) 
dim(testing)
```

*	Boruta feature selection algorithm confirmed 15 out of the 16 attributes to be important and only one attribute (default) was considered unimportant.

```{r selection}
library(Boruta)
boruta.bank_train <- Boruta(y~., data =training, doTrace = 2)
print(boruta.bank_train)
```

**Assumptions:**

*	Training and testing data follow the same distribution.

*	Dataset attributes are independent and have no correlation with each other.

\newpage

# EXPERIMENT

## EXPLORATORY DATA ANALYSIS

```{r  numeric histogram}
par(mfrow = c(3,2))
hist(bankdata$age)
hist(bankdata$duration)
hist(bankdata$balance)
hist(bankdata$campaign)
hist(bankdata$previous)
hist(bankdata$pdays)
```

__FIGURE 1: NUMERIC Attributes HISTOGRAM__

Figure 1 shows that the Histogram of all numeric attributes except for the attribute day (Figure 2) show that both attributes are skewed to the right which indicates the Mean < Median < Mode and that the lower bounds of these attributes are relatively low to the rest of the data. 
```{r  day histogram}
hist(bankdata$day)
```

__FIGURE 2: DAY Attribute HISTOGRAM__

```{r Accpeted/not Accepted}
library(ggplot2)
ggplot(data=bankdata, aes(x=y, fill=y)) + geom_bar(stat="count")
```

__FIGURE 3: OUTPUT VARIABLE BAR PLOT__

The outcome-y bar plot shows that the super majority of the clients involved in the campaign did not subscribe to a term deposit. This raises the question about the balance of the dataset and the effect it has on the classification techniques.

\newpage

## Decision Tree

### Model Description

Decision Trees is a model outlined in the shape of a tree. It consists of nodes that characterize certain features while the branches represent a selection of values. It starts with the root node highlighting the feature that has the largest effect on the classification decision, branches into different internal nodes by splitting the values of these features, and finishes the process with a leaf node which decides the class or the output variable (Ali et al, 2012).

### Model Results

```{r Decision Tree}
set.seed(0)
intrain <- createDataPartition(y = bankdata$y, p= 0.7, list = FALSE)
training <- bankdata[intrain,]
testing <- bankdata[-intrain,]
dim(training) 
dim(testing)
trctrl <- trainControl(method = "repeatedcv", number = 10, repeats = 3)
set.seed(0)
dtree_fit <- train(y ~., data = training, method = "rpart",
                   parms = list(split = "information"),
                   trControl=trctrl,
                   tuneLength = 10)
print(dtree_fit)
prp(dtree_fit$finalModel, box.palette = "Reds", tweak = 1.2)
```

__FIGURE 4: DECISION TREE MODEL REPRESENTATION__

```{r Decision Tree CM}
dt_pred <- predict(dtree_fit, newdata = testing)
conf_mat=confusionMatrix(dt_pred,as.factor(testing$y))
print(conf_mat)

```

__FIGURE 5: DECISION TREE CONFUSION MATRIX & STATISTICS__

\newpage

## Naive Bayes

### Model Description

The  Naive  Bayes  algorithm  is  a probability  classifier  that  computes  a  set  of  probabilities  by  calculating  the  frequency  and  combinations  of  values  in  a  given  data  set. Bayes theorem is utilized in this model under the assumption of the independence of the all the variables considering the value of the output variable. The problem is that this conditional assumption is seldom legitimate for real-world applications, hence the Naïve characterization, but the algorithm  manages to learn quickly in a selection of classification problems (Saritas and Yasar, 2019).


### Model Results
```{r Naive}

naive_fit <- naiveBayes(y ~ ., data = training) 
naive_pred <- predict(naive_fit, newdata = testing) 

conf_mat=confusionMatrix(naive_pred,as.factor(testing$y))
print(conf_mat)

```
__FIGURE 6: NAIVE BAYES CONFUSION MATRIX & STATISTICS__

\newpage

## K Nearest Neighbor

### Model Description

K-nearest neighbor algorithm is a method for categorizing objects centered on the nearest training data instances in the feature space. It has one of the simplest machine learning algorithms’ builds. The Training partition of this algorithm involves collecting attribute vectors with the labels of the selected attributes. The uncategorized data point is assigned to the label of its k nearest neighbors by the majority vote of all the data points involved (Kim et al, 2012).

### Model Results

```{r KNN}

set.seed(0)

intrain <- createDataPartition(y = bankdataP$y, p= 0.7, list = FALSE)
training <- bankdataP[intrain,]
testing <- bankdataP[-intrain,]

knn_fit <- knn(train = training, test = testing,   cl = training$y, k = 5)
conf_mat=confusionMatrix(knn_fit,as.factor(testing$y))
print(conf_mat)


print(cm)
```

__FIGURE 7: KNN CONFUSION MATRIX & STATISTICS__

```{r KNN K Index}
acc=array()
for (i in 1:30)
  {
  knn_fit <- knn(train = training, test = testing,   cl = training$y, k = i)
  accuracy <- mean(knn_fit == testing$y)
  acc[i]=accuracy
}
x=c(1:30)
y=acc
plot(acc)

```

__FIGURE 7: KNN K INDEX & ACCURACY__

\newpage

## RandomForest

### Model Description

Random Forest is an ensemble classification trees model made from the random selection of samples of the dataset training data with features randomly selected in the process. An outcome prediction is produced by totalling the majority vote for the predictions of the individual trees. Random Forest usually presents a considerable performance increase compared to a decision tree model; however, it is dependent on the sampling method (Ali et al, 2012).

### Model Results

```{r Randon Forest}
library(randomForest)
training$y <- as.factor(training$y)

rf_fit <-randomForest(y ~ .,  
                        data = training,  
                        importance = TRUE) 
rf_pred=predict(rf_fit,testing)
conf_mat=confusionMatrix(rf_pred,as.factor(testing$y))
print(conf_mat)
```

__FIGURE 7: RANDOM FOREST CONFUSION MATRIX & STATISTICS__

\newpage

# ANALYSIS

__TABLE 2: MODEL STATISTICS COMPARISON__

```{r table2}
ModelComparison = read.table('ModelComparison.csv',sep=',',header = T)
colnames(ModelComparison) = c('MODEL', 'ACCURACY','SENSITIVITY','SPECIFICITY','BALANCED ACCURACY')
kable(ModelComparison) 
```

Based on the results of the classification models, Decision trees showed the highest accuracy based of all three individual classification algorithms. They all had similar sensitivity and model statistics. It was also noticed that when random forest ensemble model was used, it did not raise the accuracy or sensistivity by a considerable margin. However, due to the dataset being imbalanced towards a (No) outcome in the output variable and so it is also important to look at the balanced accuracy where the effect of using an ensemble model such as the random forest can be highlighted with it having the highest balanced accuracy of 69.39%.


# CONCLUSION

Decision trees is arguably the best suited for this application since the model does not require normalization of the data, the result is easier to infer information from, and it has a built-in feature selection inside the algorithm itself which make it require less data preprocessing. The main attributes that were repeated multiple times in the decision tree were the duration of the last contact with client and the outcome of the previous campaign. This helps management team allocate resources and focus on the variables that would help with future campaigns.

\newpage

# REFERENCES

*	Ali, J., Khan, R., Ahmad, N. and Maqsood, I. (2012). Random forests and decision trees. International Journal of Computer Science Issues (IJCSI), 9(5), p.272. Available at: [https://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.402.3863&rep=rep1&type=pdf](https://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.402.3863&rep=rep1&type=pdf) 

*	Karim, M., and Rahman, R.M. (2013). Decision tree and naive bayes algorithm for classification and generation of actionable knowledge for direct marketing. Available at: [https://www.scirp.org/html/6-9301587_30463.htm](https://www.scirp.org/html/6-9301587_30463.htm) 

*	Kim¹, J.I.N.H.O., Kim, B.S. and Savarese, S. (2012). Comparing image classification methods: K-nearest-neighbour and support-vector-machines. In Proceedings of the 6th WSEAS international conference on Computer Engineering and Applications, and Proceedings of the 2012 American conference on Applied Mathematics (Vol. 1001, pp. 48109-2122). Available at: 
[http://www.wseas.us/e-library/conferences/2012/CambridgeUSA/MATHCC/MATHCC-18.pdf](http://www.wseas.us/e-library/conferences/2012/CambridgeUSA/MATHCC/MATHCC-18.pdf) 

*	Moro, S., Laureano, R. and Cortez, P. (2011). Using data mining for bank direct marketing: An application of the crisp-dm methodology. Available at: [https://repositorium.sdum.uminho.pt/handle/1822/14838](https://repositorium.sdum.uminho.pt/handle/1822/14838) 

*	Moro, S., Cortez, P. and Rita, P. (2014). A data-driven approach to predict the success of bank telemarketing. Decision Support Systems, 62, pp.22-31. Available at: [https://www.sciencedirect.com/science/article/abs/pii/S016792361400061X?casa_token=aCFwfGCIS0AAAAA:YSQ4yvfrGhlqNaw1LNKZlkxLD0BsbslPd0Wgjdc7VkyLzdQxi5O3NOGdWiGpWtNi81A_JR3Efiw](https://www.sciencedirect.com/science/article/abs/pii/S016792361400061X?casa_token=aCFwfGCIS0AAAAA:YSQ4yvfrGhlqNaw1LNKZlkxLD0BsbslPd0Wgjdc7VkyLzdQxi5O3NOGdWiGpWtNi81A_JR3Efiw) 
Dataset at: [https://archive.ics.uci.edu/ml/datasets/Bank+Marketing](https://archive.ics.uci.edu/ml/datasets/Bank+Marketing)

*	Moro, Sérgio & Cortez, Paulo & Laureano, Raul. (2012). Enhancing Bank Direct Marketing through Data Mining. CAlg European Marketing Academy. Available at: [https://repositorium.sdum.uminho.pt/handle/1822/21409](https://repositorium.sdum.uminho.pt/handle/1822/21409)

*	Patel, R.S. and Mazumdar, H.S. (2018). Prediction of Bank Investors using Neural Network in Direct Marketing. International Journal of Engineering and Applied Sciences, 5(2). Available at: [https://www.neliti.com/publications/257283/prediction-of-bank-investors-using-neural-network-in-direct-marketing#cite](https://www.neliti.com/publications/257283/prediction-of-bank-investors-using-neural-network-in-direct-marketing#cite)

*	Pavlović, D., Reljić, M. and Jaćimović, S. (2014). Application of data mining in direct marketing in banking sector. Industrija, 42(1), pp.189-201. Available at: [https://core.ac.uk/download/pdf/79431224.pdf](https://core.ac.uk/download/pdf/79431224.pdf)

* Saritas, M.M. and Yasar, A. (2019). Performance analysis of ANN and Naive Bayes classification algorithm for data classification. International Journal of Intelligent Systems and Applications in Engineering, 7(2), pp.88-91. Available at: 
[https://ijisae.org/IJISAE/article/view/934/585](https://ijisae.org/IJISAE/article/view/934/585)

*	Thomas, B., & Housden, M. (2002). Direct marketing in practice. Oxford, Butterworth-Heinemann. Available at: [https://books.mec.biz/tmp/books/BZUNEGF6GMCFB6FGXYG6.pdf](https://books.mec.biz/tmp/books/BZUNEGF6GMCFB6FGXYG6.pdf) 

*	Wisaeng, K. (2013). A Comparison of Different Classification Techniques for Bank Direct Marketing. Available at: [https://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.412.9849&rep=rep1&type=pdf](https://books.mec.biz/tmp/books/BZUNEGF6GMCFB6FGXYG6.pdf)

